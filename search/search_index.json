{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"chemistry_MEP_TS_Optimization","text":"<p>Welcome to the documentation for the <code>chemistry_MEP_TS_optimization</code> code! This python package provides functionality to obtain minimum energy path (MEP) connecting two minima (namely, reactant and product for a chemical system) which passes through a saddle point (called, the transition state for a chemical system).</p>"},{"location":"example_docs/about/changelog.html","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"example_docs/about/changelog.html#002","title":"[0.0.2]","text":""},{"location":"example_docs/about/changelog.html#added","title":"Added","text":"<ul> <li>Added <code>.pre-commit-config.yaml</code></li> </ul>"},{"location":"example_docs/about/changelog.html#changed","title":"Changed","text":"<ul> <li>Use <code>ruff</code> for formatting</li> </ul>"},{"location":"example_docs/about/changelog.html#001","title":"[0.0.1]","text":""},{"location":"example_docs/about/changelog.html#added_1","title":"Added","text":"<ul> <li>The initial release!</li> </ul>"},{"location":"example_docs/about/conduct.html","title":"Code of Conduct","text":""},{"location":"example_docs/about/conduct.html#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"example_docs/about/conduct.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"example_docs/about/conduct.html#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"example_docs/about/conduct.html#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"example_docs/about/conduct.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"example_docs/about/conduct.html#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"example_docs/about/conduct.html#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"example_docs/about/conduct.html#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"example_docs/about/conduct.html#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"example_docs/about/conduct.html#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"example_docs/about/conduct.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"example_docs/about/license.html","title":"License","text":"LICENSE.md<pre><code>BSD 3-Clause License\n\nCopyright (c) 2024\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived from\n   this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"example_docs/code/hints.html","title":"Type Hinting","text":""},{"location":"example_docs/code/hints.html#overview","title":"Overview","text":"<p>In the sample functions provided with the template repository, you will see something like:</p> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n</code></pre> <p>If you aren't familiar with type-hinting, that's what the <code>: float</code>, <code>: int</code>, and <code>-&gt; NDArray</code> are indicating. They tell the user what the expected types are for each parameter and return. They are not enforced in any way; they are merely hints (as the name suggests). It is always advisable to use type hints in your code, so get in the habit of doing so!</p> <p>Tip</p> <p>If you have to import a given function solely for type-hinting purposes, you should put it within an <code>if TYPE_CHECKING</code> block (as demonstrated in <code>/src/template/examples/sample.py</code>). It will then only be imported when using a type-checking utility, reducing the overall import time of your module.</p> <p>Note</p> <p>You do not need to touch the <code>py.typed</code> file. It is a marker that Python uses to indicate that type-hinting should be used in any programs that depend on your code.</p>"},{"location":"example_docs/code/hints.html#type-checking","title":"Type Checking","text":"<p>As mentioned, the type hints are just that: hints. If you want to ensure that the types are strictly adhered to across your codebase, you can use mypy to do so. This is a slightly more advanced tool, however, so is not something you need to worry about right now.</p>"},{"location":"example_docs/code/source.html","title":"Source Code","text":""},{"location":"example_docs/code/source.html#adding-your-code","title":"Adding Your Code","text":"<p>All source code (i.e. your various modules, functions, classes, and so on) should be placed in the <code>/src/&lt;MyPackageName&gt;</code> directory. A sample file named <code>examples/sample.py</code> is included here as a representative example, which you should replace.</p> <p>All the code in the <code>src</code> directory can be imported now that you have installed your package.</p> <p>Tip</p> <p>As an example, you can import and use the demonstration template.examples.sample functions as follows:</p> <pre><code>from MyPackageName.examples.sample import add, make_array\n\nprint(add(1, 2))  # 3\nprint(make_array(3, length=4))  # [3, 3, 3, 3]\n</code></pre> <p>Note</p> <p>For any subfolder within <code>src/&lt;MyPackageName&gt;</code> containing Python code, you must have an <code>__init__.py</code> file, which will tell Python that this is a module you can import.</p>"},{"location":"example_docs/code/source.html#docstrings","title":"Docstrings","text":"<p>The code comments beneath each function are called docstrings. They should provide an overview of the purpose of the function, the various parameters, and the return values (if any). Here, we are using the NumPy style docstrings, but you can pick a different style if you like later on.</p>"},{"location":"example_docs/code/tests.html","title":"Testing","text":""},{"location":"example_docs/code/tests.html#overview","title":"Overview","text":"<p>Writing effective tests for your code is a crucial part of the programming process. It is the best way to ensure that changes you make to your codebase throughout the development process do not break the core functionality of your code. This may be your first time writing tests, but trust me that it is essential.</p>"},{"location":"example_docs/code/tests.html#pytest","title":"Pytest","text":"<p>Put any unit tests in the <code>/tests</code> folder. A sample test (i.e. <code>/tests/sample/examples/test_sample.py</code>) is included as a representative example.</p> <p>Note</p> <p>All your testing scripts should start with <code>test_</code> in the filename.</p> <p>When you installed the package with the <code>[dev]</code> extras, you installed everything you need to run your unit tests. To run the unit tests locally, run <code>pytest .</code> in the base directory. It will let you know if any tests fail and what the reason is for each failure.</p>"},{"location":"example_docs/github/commits.html","title":"Saving Your Work","text":"<p>There are still a few more steps left, but at this point you will want to make sure to save your work!</p>"},{"location":"example_docs/github/commits.html#pushing-your-changes","title":"Pushing Your Changes","text":"<p>Commit any changes you've made and push them to your repository</p> <p>If you are using a program like GitKraken, this will involve the following steps:</p> <ol> <li>Save your work.</li> <li>Recommended: make a new branch for your work (e.g. <code>develop</code>)</li> <li>Click \"Stage all changes\".</li> <li>Add a helpful commit message.</li> <li>Commit the changes.</li> <li>Click \"push\".</li> </ol> <p>Tip</p> <p>It is advisable to make changes in a new branch rather than in <code>main</code> so that you can ensure your unit tests pass before the code is merged into the codebase.</p> <p>Then go on GitHub to see your changes. Assuming you pushed your changes to a new branch, you'll likely see a message asking if you want to make a Pull Request to merge in your changes into the <code>main</code> branch.</p>"},{"location":"example_docs/github/workflows.html","title":"GitHub Actions","text":""},{"location":"example_docs/github/workflows.html#workflows","title":"Workflows","text":"<p>The last major piece of the puzzle is GitHub Actions, which is an automated suite of workflows that run every time a commit or pull request is made. The GitHub workflows can be found in the <code>.github/workflows</code> folder.</p>"},{"location":"example_docs/github/workflows.html#tests","title":"Tests","text":"<p>The <code>/.github/workflows/tests.yaml</code> file contains the workflow to have GitHub automatically run the full suite of tests on every commit and pull request. For the most basic case outlined here, you do not need to make any modifications (other than, perhaps, the desired Python versions you wish to test on).</p> <p>By default, the test suite is set up to install the following packages:</p> <pre><code>pip install -r tests/requirements.txt\npip install .[dev]\n</code></pre> <p>As you can see above, it will install specific versions of the dependencies outlined in <code>/tests/requirements.txt</code>. Unlike <code>pyproject.toml</code>, you want to include specific versions here so that your test suite is reproducible.</p> <p>The <code>/.github/dependabot.yml</code> file is set up such that Dependabot will automatically open pull requests to update any versions in your <code>/tests/requirements.txt</code> file as they come out so that your code will always be tested on the newest releases of the various dependencies. This will ensure that your code doesn't break as dependencies update, but if it does, you will know what needs fixing.</p>"},{"location":"example_docs/github/workflows.html#documentation","title":"Documentation","text":"<p>The <code>/.github/workflows/docs.yaml</code> file contains the workflow to have GitHub test the build process for the documentation and deploy it (if enabled).</p> <p>To have your documentation automatically deployed on a GitHub webpage:</p> <ol> <li>Go to the settings page of your repository.</li> <li>Click the \"Pages\" section under \"Code and automation.\"</li> <li>Select \"Deploy from a branch\" under \"Source\"</li> <li>Set the branch to be \"gh-pages\" with \"/ (root)\" as the folder.</li> <li>Wait a minute and refresh the page. You'll see a message that your site is live with a URL to the documentation.</li> </ol> <p></p> <p>Once this process is done, the documentation will be live and will update with each commit.</p>"},{"location":"example_docs/github/workflows.html#release","title":"Release","text":"<p>The <code>/.github/workflows/release.yaml</code> file contains the workflow to have GitHub upload your package to PyPI every time you mint a new release on GitHub. This is a slightly more advanced topic that you can read more about at a later time, but it's there for when you need it.</p>"},{"location":"example_docs/installation/install.html","title":"Pip Installing","text":"<p>Now it's time to install your Python package! You will want to install your Python package in \"editable\" mode, which means you won't have to re-install your code every time you make updates to it. Additionally, you will want to install several optional dependencies (listed under the <code>[project.optional-dependencies]</code> header in <code>pyproject.toml</code>) to ensure that you can test and build the documentation for your code.</p> <p>With all this in mind, you will want to run the following in the command line from the base of the package directory:</p> <pre><code>pip install -e .[dev,docs]\n</code></pre> <p>Here, the <code>-e</code> means editable mode, the <code>.</code> means the current directory, and the <code>[dev,docs]</code> means it will install the \"dev\" and \"docs\" optional dependency set listed in the <code>pyproject.toml</code> file.</p> <p>Tip</p> <p>You should generally start from a clean Python environment, such as a new Conda environment if you are using Anaconda or one of its variants.</p> <p>To make sure you installed your package successfully, open a Python console and run <code>import &lt;MyPackageName&gt;</code>. It should return without any errors. If there are errors, it's likely because you forgot to replace a \"template\" placeholder with the name of your package.</p>"},{"location":"example_docs/installation/pyproject.html","title":"pyproject.toml","text":"<p>The <code>pyproject.toml</code> file contains all of the necessary information on how Python will install your package.</p>"},{"location":"example_docs/installation/pyproject.html#metadata","title":"Metadata","text":"<p>There are several metadata-related fields that you will likely want to update. You should have already updated the <code>name</code> of the package in a prior step when you replaced \"template\" everywhere, but you will also want to change the following:</p> <ul> <li><code>description</code></li> <li><code>license</code> (if you changed the default <code>LICENSE.md</code> file)</li> <li><code>authors</code></li> <li><code>keywords</code></li> </ul> <p>Aside from the <code>name</code>, none of the above are strictly necessary and can be left as-is (or removed) if you are unsure.</p>"},{"location":"example_docs/installation/pyproject.html#dependencies","title":"Dependencies","text":"<p>The most important fields to update are related to the dependencies: the Python packages that your own code relies on. This will ensure that they are automatically installed when installing your Python package.</p> <p>The required dependencies are listed under the <code>[project]</code> header in the <code>dependencies</code> field. By default, the template repository lists <code>[\"numpy\"]</code>. Include any dependencies you want in this list, separated by commas. This should be all the packages you import in your code that are not standard Python libraries.</p> <p>Tip</p> <p>Not sure what dependencies you need just yet? No problem. You can come back to this later.</p> <p>Note</p> <p>If you know a specific minimum version is needed for your code, you should set that here as well (e.g. <code>[\"numpy&gt;=1.23.0\"]</code>). However, only use this when it is necessary so that users aren't restricted to a given version without a valid reason.</p>"},{"location":"example_docs/installation/pyproject.html#python-version","title":"Python Version","text":"<p>If you know your code can only run on certain Python versions, you should specify that in the <code>requires-python</code> field under the <code>[project]</code> header. When in doubt, we recommend setting it to the range of currently supported Python versions (specifically those with security and bugfix statuses).</p> <p>You can also update the listed versions in the <code>classifiers</code> field, although this is only for informational purposes. The list of supported Python classifier fields can be found on the corresponding PyPI page.</p>"},{"location":"example_docs/intro/resources.html","title":"Resources","text":""},{"location":"example_docs/intro/resources.html#software-development","title":"Software Development","text":"<p>Looking for external resources to get started with software development? Here are some useful ones:</p> <ul> <li>Scientific Python Development Guide</li> <li>Turing Way Guide for Reproducible Research</li> <li>Turing Way Guide for Project Design</li> </ul>"},{"location":"example_docs/intro/resources.html#git-and-version-control","title":"Git and Version Control","text":"<p>For git and version control specifically:</p> <ul> <li>Git Guides</li> <li>Software Carpentry</li> </ul> <p>For GitHub:</p> <ul> <li>GitHub Docs</li> <li>GitHub Skills</li> </ul>"},{"location":"example_docs/intro/why.html","title":"Why?","text":""},{"location":"example_docs/intro/why.html#purpose","title":"Purpose","text":"<p>The first question to address is: why? Why use a template repository like this? Why make a Python package at all, as opposed to writing custom scripts or Jupyter Notebooks?</p> <p>The answer, in short, is sustainable and reproducible software development. Here are some of the benefits:</p> <ul> <li>Your package can be easily installed by others using <code>pip</code>.</li> <li>Your package can have automated unit tests that run every time you make a commit, making sure you don't accidentally break your own code.</li> <li>You can easily make and share documentation with no hassle.</li> <li>You will instantly be adopting good programming practices that will help you for life.</li> </ul> <p>Of course, there are many more reasons, but hopefully that's convincing enough!</p>"},{"location":"example_docs/intro/why.html#alternatives","title":"Alternatives","text":"<p>This is by no means the only template of its kind. Some alternatives include:</p> <ul> <li>cookiecutter</li> <li>pyscaffold</li> <li>python-package-template</li> </ul> <p>... and many more.</p> <p>Feel free to use them if you wish! This template repository exists because we are all opinionated people, and this template focuses on things that I value most. But the point is to just use something that works well for you.</p>"},{"location":"example_docs/mkdocs/build.html","title":"Building the Docs","text":""},{"location":"example_docs/mkdocs/build.html#the-mkdocsyml-file","title":"The <code>mkdocs.yml</code> File","text":"<p>Once you have added your documentation, you will need to update the <code>/mkdocs.yml</code> file with information about how you want to arrange the files. Specifically, you will need to update the <code>nav</code> secction of the <code>mkdocs.yml</code> file to point to all your individual <code>.md</code> files, organizing them by category.</p> <p>Note</p> <p>Keep the <code>- Code Documentation: reference/</code> line in the <code>nav</code> section of <code>mkdocs.yml</code>. It will automatically transform your docstrings into beautiful documentation! The rest of the <code>nav</code> items you can replace.</p>"},{"location":"example_docs/mkdocs/build.html#the-build-process","title":"The Build Process","text":"<p>To see how your documentation will look in advance, you can build it locally by running the following command in the base directory:</p> <pre><code>mkdocs serve\n</code></pre> <p>A URL will be printed out that you can open in your browser.</p>"},{"location":"example_docs/mkdocs/build.html#deploying-the-docs","title":"Deploying the Docs","text":"<p>To allow your documentation to be visible via GitHub Pages, go to \"Settings &gt; Pages\" in your repository's settings and make sure \"Branch\" is set to \"gh-pages\" instead of \"main\".</p>"},{"location":"example_docs/mkdocs/docs.html","title":"Writing the Docs","text":""},{"location":"example_docs/mkdocs/docs.html#mkdocs","title":"Mkdocs","text":"<p>Now it's time to write some documentation! This isn't very difficult, and of course you're reading some documentation right now. The documentation is written using markdown, which is the same way GitHub comments are formatted.</p> <p>Tip</p> <p>Check out the Markdown Guide for an overview of the basic syntax.</p> <p>This template repository uses a documentation format called mkdocs, specifically a useful theme called Material for Mkdocs. This enables many wonderful goodies like the \"tip\" callout you see above and much more.</p>"},{"location":"example_docs/mkdocs/docs.html#adding-markdown-files","title":"Adding Markdown Files","text":"<p>Your documentation will live in the <code>/docs</code> folder. You can think of each markdown (<code>.md</code>) file as being a specific page in the documentation, and each folder as being a related collection of pages. The markdown page you are reading right now is found at <code>/docs/example_docs/mkdocs/docs.md</code>, for instance. Of course, you will want to replce the <code>/docs/example_docs</code> folder with your own documentation.</p> <p>Note</p> <p>You typically do not need to touch the <code>/docs/gen_ref_pages.py</code> script. It is used to automatically build the documentation for your code from its docstrings.</p>"},{"location":"example_docs/other/apps.html","title":"Third-Party Apps","text":""},{"location":"example_docs/other/apps.html#overview","title":"Overview","text":"<p>There are several configuration files provided for popular third-party applications that can be quite useful:</p> <ol> <li>The <code>/.deepsource.toml</code> file is a configuration file for the web platform DeepSource, which has a useful GitHub extension for cleaning up your code and spotting common errors.</li> <li>The <code>/.sourcery.yaml</code> file is a configuration file for Sourcery, which can automatically refactor your code.</li> <li>The <code>/.codecov.yml</code> file is a configuration file for Codecov, which will tell you the fraction of lines covered by your test suite if the GitHub integration is enabled.</li> </ol>"},{"location":"example_docs/other/linting.html","title":"Linting and Formatting","text":""},{"location":"example_docs/other/linting.html#overview","title":"Overview","text":"<p>When you installed the <code>[dev]</code> dependencies, you installed several code-formatting and linting tools, including:</p> <ol> <li><code>black</code>: A very useful and opinionated code formatter, which you can use by running <code>black .</code> in the base directory.</li> <li><code>isort</code>: A utility that will sort your import statements for you, which you can use by running <code>isort .</code> in the base directory.</li> <li><code>ruff</code>: A versatile Python linter to clean up your code, which you can use by running <code>ruff . --fix</code> in the base directory.</li> <li><code>docformatter</code>: A simple docstring formatter, which you can use by running <code>docformatter . -r -i</code> in the base directory.</li> </ol> <p>Modifications to the rules these formatters use can be defined in the <code>pyproject.toml</code> file, and we have chosen some useful defaults.</p>"},{"location":"example_docs/setup/basics.html","title":"Initial Changes","text":"<p>At this point, you now have your template repository on GitHub and locally on your machine. Now it's time to start making some modifications.</p>"},{"location":"example_docs/setup/basics.html#readme","title":"README","text":"<p>The first thing to do is update the README (<code>/README.md</code>), which should contain a user-friendly summary of what your package is all about. This can be whatever you want. Feel free to be creative!</p>"},{"location":"example_docs/setup/basics.html#license","title":"License","text":"<p>The template repository comes premade with a sample license (<code>/LICENSE.md</code>), in this case the very popular and permissive BSD 3-Clause license. Feel free to change this for your own project or keep it as-is if you don't quite know yet.</p> <p>Tip</p> <p>There are many licenses that one can consider. A comprehensive list can be found on the Open Source Initiative website, but a less overwhelming route is to use choosealicense.com.</p>"},{"location":"example_docs/setup/basics.html#code-of-conduct","title":"Code of Conduct","text":"<p>The template repository ships with a premade Code of Conduct (<code>/CODE_OF_CONDUCT.md</code>) that is obtained from the Contributor Covenant. Of course, you can feel free to keep or change this as you see fit, but it is often a good idea to have a code of conduct for public repositories.</p>"},{"location":"example_docs/setup/name.html","title":"Updating the Name","text":"<p>Now for your first major task: replace all instances of the word \"template\" with your desired package name.</p> <p>Note</p> <p>Don't forget to update the name of the <code>/src/template</code> folder, e.g. so that it is of the form <code>src/&lt;MyPackageName&gt;</code>.</p> <p>Tip</p> <p>If you're using Visual Studio Code as your editor, you can do <code>ctrl+shift+H</code> to find-and-replace all instances of \"template\" with your own package name.</p> <p></p>"},{"location":"example_docs/setup/prep.html","title":"Preparatory Steps","text":""},{"location":"example_docs/setup/prep.html#naming-your-package","title":"Naming Your Package","text":"<p>So, you have an idea for your own Python package. The first thing you'll need to do is come up with a name!</p> <p>Tip</p> <p>If you plan on making a Python package that is widely distributed, first check to see if the name already exists on PyPI.</p>"},{"location":"example_docs/setup/prep.html#making-a-repository","title":"Making a Repository","text":"<p>With a nice name in mind, create a new repository using this template. Give it a name, a description, and decide if you want it to be public or private.</p> <p></p>"},{"location":"example_docs/setup/prep.html#cloning-your-repository","title":"Cloning Your Repository","text":"<p>You'll now want to clone the repository to your local machine so you can easily make changes.</p>"},{"location":"example_docs/setup/prep.html#via-a-desktop-client","title":"Via a Desktop Client","text":"<p>You can use a desktop client to interface with GitHub. It is worthwhile to learn how to use such a program for your day-to-day work.</p> <p>Tip</p> <p>We strongly suggest using GitKraken to interface with git and GitHub. GitKraken Pro is also free for students.</p> <p></p>"},{"location":"example_docs/setup/prep.html#via-the-command-line","title":"Via the Command Line","text":"<p>If you prefer, you can clone the repository via the following command in the command-line, provided you have git installed.</p> <pre><code>git clone https://github.com/MyAccountName/MyPackageName\n</code></pre> <p>You can get the URL directly from the GitHub page when you click the green \"&lt;&gt; Code\" button.</p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>chemistry_MEP_TS_optimization<ul> <li>dmitriy</li> <li>examples<ul> <li>sample</li> </ul> </li> <li>mechanics<ul> <li>action</li> <li>lagrangians</li> </ul> </li> <li>optimization<ul> <li>gradient_descent</li> <li>gradient_descent_discrete</li> <li>gradient_descent_orig</li> <li>initialize_path</li> <li>losses</li> <li>path_metrics_orig</li> <li>path_optimizer</li> <li>update_minima</li> </ul> </li> <li>paths<ul> <li>b_spline</li> <li>base_path</li> <li>elastic_band</li> <li>initialize</li> <li>mlp</li> </ul> </li> <li>playground<ul> <li>eqx_example</li> <li>example</li> <li>split_diff_static</li> <li>test_integrator</li> </ul> </li> <li>potentials<ul> <li>base_class</li> <li>constant</li> <li>muller_brown</li> <li>wolfe_schlegel</li> </ul> </li> <li>run_optimization</li> <li>run_optimization_orig</li> <li>tools<ul> <li>arg_parser</li> <li>configs</li> <li>integrator</li> <li>logging</li> <li>metrics</li> <li>visualize</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/chemistry_MEP_TS_optimization/dmitriy.html","title":"dmitriy","text":""},{"location":"reference/chemistry_MEP_TS_optimization/dmitriy.html#chemistry_MEP_TS_optimization.dmitriy.f","title":"f","text":"<pre><code>f(t)\n</code></pre> <p>A function that returns square of a number.</p> <p>Args:     t (float): Input value.</p> <p>Returns:     float: Output value.</p> Source code in <code>chemistry_MEP_TS_optimization/dmitriy.py</code> <pre><code>def f(t: float) -&gt; float:\n    \"\"\"\n    A function that returns square of a number.\n\n    Args:\n        t (float): Input value.\n\n    Returns:\n        float: Output value.\n    \"\"\"\n    return t**2\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/dmitriy.html#chemistry_MEP_TS_optimization.dmitriy.vector_field","title":"vector_field","text":"<pre><code>vector_field(t, x, y)\n</code></pre> <p>Defines the vector field for the ordinary differential equation.</p> <p>Args:     t (float): Time.     x (float): First coordinate.     y (float): Second coordinate.</p> <p>Returns:     float: Result of the vector field computation.</p> Source code in <code>chemistry_MEP_TS_optimization/dmitriy.py</code> <pre><code>def vector_field(t: float, x: float, y: float) -&gt; float:\n    \"\"\"\n    Defines the vector field for the ordinary differential equation.\n\n    Args:\n        t (float): Time.\n        x (float): First coordinate.\n        y (float): Second coordinate.\n\n    Returns:\n        float: Result of the vector field computation.\n    \"\"\"\n    print(t,x,y)\n    return jnp.linalg.norm(jac(t))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/run_optimization.html","title":"run_optimization","text":""},{"location":"reference/chemistry_MEP_TS_optimization/run_optimization.html#chemistry_MEP_TS_optimization.run_optimization.run_opt","title":"run_opt","text":"<pre><code>run_opt(args, config, path_config, logger)\n</code></pre> <p>Run optimization process.</p> <p>Args:     args (NamedTuple): Command line arguments.     config (NamedTuple): Configuration settings.     path_config (NamedTuple): Path configuration.     logger (NamedTuple): Logger settings.</p> Source code in <code>chemistry_MEP_TS_optimization/run_optimization.py</code> <pre><code>def run_opt(\n        args: NamedTuple,\n        config: NamedTuple,\n        path_config: NamedTuple,\n        logger: NamedTuple\n):\n    \"\"\"\n    Run optimization process.\n\n    Args:\n        args (NamedTuple): Command line arguments.\n        config (NamedTuple): Configuration settings.\n        path_config (NamedTuple): Path configuration.\n        logger (NamedTuple): Logger settings.\n    \"\"\"\n    # Create output directories\n    output_dir = os.path.join(args.output_dir, config.potential, config.optimizer)\n    log_dir = os.path.join(output_dir, \"logs\")\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    #####  Get chemical potential  #####\n    potential = get_potential(\n        config.potential,\n        tag=config.potential_tag,\n        expect_config=config.potential!=\"constant\",\n        add_azimuthal_dof=args.add_azimuthal_dof,\n        add_translation_dof=args.add_translation_dof\n    )\n\n    # Minimize initial points with the given potential\n    if args.minimize_end_points:\n        minima_finder = optimization.MinimaUpdate(potential)\n        minima = minima_finder.find_minima(\n            [config.initial_point, config.final_point]\n        )\n        print(f\"Optimized Initial Point: {minima[0]}\")\n        print(f\"Optimized Final Point: {minima[1]}\")\n        sys.exit(0)\n\n    #####  Get path prediction method  #####\n    path = paths.get_path(\n        config.path,\n        potential,\n        config.initial_point,\n        config.final_point,\n        #add_azimuthal_dof=args.add_azimuthal_dof,\n        #add_translation_dof=args.add_translation_dof,\n        **path_config.path_params\n    )\n\n    # Randomly initialize the path, otherwise a straight line\n    if args.randomly_initialize_path is not None:\n        path = optimization.randomly_initialize_path(\n            path, args.randomly_initialize_path\n        )\n\n    #####  Path optimization tools  #####\n    # Path integrating function\n    integrator = tools.ODEintegrator(\n        potential,\n        solver=config.integral_params['solver'],\n        rtol=config.integral_params['rtol'],\n        atol=config.integral_params['atol']\n    )\n    #print(\"test integrate\", integrator.path_integral(path, 'E_pvre'))\n\n    # Gradient descent path optimizer\n    optimizer = optimization.PathOptimizer(\n        config.optimizer,\n        config.optimizer_params,\n        path,\n        config.loss_function,\n        path_type=config.path,\n        potential_type=config.potential,\n        config_tag=config.optimizer_config_tag\n    )\n\n    # Loss\n    #print(config.loss_functions)\n    #loss_grad_fxn, loss_fxn = optimization.get_loss(config.loss_functions)\n\n    ##########################################\n    #####  Optimize minimum energy path  ##### \n    ##########################################\n    geo_paths = []\n    pes_paths = []\n    t0 = timer.time()\n    for optim_idx in range(args.num_optimizer_iterations):\n        path_integral = optimizer.optimization_step(path, integrator)\n        print(f'optim_idx:, {optim_idx}, {path_integral}')\n        if optim_idx%250 == 0:\n            print(\"EVAL TIME\", (timer.time()-t0)/60)\n            path_output = logger.optimization_step(\n                optim_idx,\n                path,\n                potential,\n                path_integral,\n                plot=args.make_opt_plots,\n                geo_paths=geo_paths,\n                pes_paths=pes_paths,\n                add_azimuthal_dof=args.add_azimuthal_dof,\n                add_translation_dof=args.add_translation_dof\n            )\n\n    print(\"EVAL TIME\", (timer.time()-t0)/60)\n    # Plot gif animation of the MEP optimization (only for 2d potentials)\n    if args.make_animation:\n        geo_paths = potential.point_transform(torch.tensor(geo_paths))\n        ani_name = f\"{config.potential}_W{path_config.path_params['n_embed']}_D{path_config.path_params['depth']}_LR{config.optimizer_params['lr']}\"\n        visualize.animate_optimization_2d(\n            geo_paths, ani_name, ani_name,\n            potential, plot_min_max=(-2, 2, -2, 2),\n            levels=np.arange(-100,100,5),\n            add_translation_dof=args.add_translation_dof,\n            add_azimuthal_dof=args.add_azimuthal_dof\n        )\n    return path_integral\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/run_optimization_orig.html","title":"run_optimization_orig","text":""},{"location":"reference/chemistry_MEP_TS_optimization/examples/sample.html","title":"sample","text":""},{"location":"reference/chemistry_MEP_TS_optimization/examples/sample.html#chemistry_MEP_TS_optimization.examples.sample.add","title":"add","text":"<pre><code>add(a, b)\n</code></pre> <p>A function that adds two numbers.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>First number to add.</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>Second number to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The sum of a and b.</p> </li> </ul> Source code in <code>chemistry_MEP_TS_optimization/examples/sample.py</code> <pre><code>def add(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that adds two numbers.\n\n    Parameters\n    ----------\n    a\n        First number to add.\n    b\n        Second number to add.\n\n    Returns\n    -------\n    float\n        The sum of a and b.\n    \"\"\"\n    return a + b\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/examples/sample.html#chemistry_MEP_TS_optimization.examples.sample.divide","title":"divide","text":"<pre><code>divide(a, b)\n</code></pre> <p>A function that divides two numbers, i.e. a/b.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>The numerator</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>The denominator</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The value for a/b</p> </li> </ul> Source code in <code>chemistry_MEP_TS_optimization/examples/sample.py</code> <pre><code>def divide(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that divides two numbers, i.e. a/b.\n\n    Parameters\n    ----------\n    a\n        The numerator\n    b\n        The denominator\n\n    Returns\n    -------\n    float\n        The value for a/b\n    \"\"\"\n    if b == 0:\n        raise ValueError(\"Uh oh! The value for b should not be 0.\")\n\n    return a / b\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/examples/sample.html#chemistry_MEP_TS_optimization.examples.sample.make_array","title":"make_array","text":"<pre><code>make_array(val, length=3)\n</code></pre> <p>A function to transform a number into a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>val</code>               (<code>float</code>)           \u2013            <p>Number to turn into an array.</p> </li> <li> <code>length</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The length of the array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>An array composed of <code>val</code>.</p> </li> </ul> Source code in <code>chemistry_MEP_TS_optimization/examples/sample.py</code> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n    \"\"\"\n    A function to transform a number into a numpy array.\n\n    Parameters\n    ----------\n    val\n        Number to turn into an array.\n    length\n        The length of the array.\n\n    Returns\n    -------\n    NDArray\n        An array composed of `val`.\n    \"\"\"\n    return np.array([val] * length)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/mechanics/action.html","title":"action","text":""},{"location":"reference/chemistry_MEP_TS_optimization/mechanics/action.html#chemistry_MEP_TS_optimization.mechanics.action.action","title":"action","text":"<pre><code>action(potential, points, start, end)\n</code></pre> <p>Calculate the action for a given path using the specified potential.</p> Parameters: <p>potential : function     Function defining the potential energy. points : array_like     Array of n points along the path. start : array_like     Fixed start point. end : array_like     Fixed end point.</p> Returns: <p>float     The action for the given path.</p> Source code in <code>chemistry_MEP_TS_optimization/mechanics/action.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef action(\n        potential,     # Function defining the potential energy\n        points,        # Array of n points along the path\n        start,         # Fixed start point\n        end,           # Fixed end point\n) -&gt; float:\n    \"\"\"\n    Calculate the action for a given path using the specified potential.\n\n    Parameters:\n    -----------\n    potential : function\n        Function defining the potential energy.\n    points : array_like\n        Array of n points along the path.\n    start : array_like\n        Fixed start point.\n    end : array_like\n        Fixed end point.\n\n    Returns:\n    --------\n    float\n        The action for the given path.\n    \"\"\"\n    accumulator = lagrangian(potential, start, points[0])\n\n    accumulator += sum(jnp.array(\n        [lagrangian(potential, points[i], points[i+1])\n         for i in range(0, points.shape[0] - 1)]))\n\n    accumulator += lagrangian(potential, points[-1], end)\n\n    return accumulator\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/mechanics/lagrangians.html","title":"lagrangians","text":""},{"location":"reference/chemistry_MEP_TS_optimization/mechanics/lagrangians.html#chemistry_MEP_TS_optimization.mechanics.lagrangians.lagrangian","title":"lagrangian","text":"<pre><code>lagrangian(potential, left_point, right_point, distance_factor=100)\n</code></pre> <p>Calculate the Lagrangian for a given potential and two points.</p> Parameters: <p>potential : function     Function defining the potential energy. left_point : array_like     Left point in the configuration space. right_point : array_like     Right point in the configuration space. distance_factor : float, optional     Scaling factor for the distance term (default is 100).</p> Returns: <p>float     The Lagrangian for the given potential and points.</p> Source code in <code>chemistry_MEP_TS_optimization/mechanics/lagrangians.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef lagrangian(\n        potential,          # Function defining the potential energy\n        left_point,         # Left point\n        right_point,        # Right point\n        distance_factor=100 # Scaling factor for the distance term\n) -&gt; float:\n    \"\"\"\n    Calculate the Lagrangian for a given potential and two points.\n\n    Parameters:\n    -----------\n    potential : function\n        Function defining the potential energy.\n    left_point : array_like\n        Left point in the configuration space.\n    right_point : array_like\n        Right point in the configuration space.\n    distance_factor : float, optional\n        Scaling factor for the distance term (default is 100).\n\n    Returns:\n    --------\n    float\n        The Lagrangian for the given potential and points.\n    \"\"\"\n    displacement = right_point - left_point\n    squares = displacement * displacement\n    graph_component = (potential(right_point) - potential(left_point)) ** 2\n    return ( jnp.exp(distance_factor*squares.sum()) - 1.0 +\n        graph_component )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent.html","title":"gradient_descent","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent.html#chemistry_MEP_TS_optimization.optimization.gradient_descent.gradientDescent","title":"gradientDescent","text":"<pre><code>gradientDescent(path, integrator, loss_fxn, config, max_n_steps=int(1000000000.0))\n</code></pre> <p>Gradient descent optimizer.</p> <p>Attributes:     path (object): The path to optimize.     integrator (object): The integrator to compute gradients.     loss_fxn (callable): The loss function to minimize.     config (dict): Configuration parameters.     max_n_steps (int): Maximum number of optimization steps.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent.py</code> <pre><code>def __init__(\n        self,\n        path,\n        integrator,\n        loss_fxn,\n        config,\n        max_n_steps: int = int(1e9),\n) -&gt; None:\n    self.path = path\n    self.integrator = integrator\n    self.loss_fxn = loss_fxn\n    self.config = config\n    self.max_n_steps = max_n_steps\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent.html#chemistry_MEP_TS_optimization.optimization.gradient_descent.gradientDescent.find_critical_path","title":"find_critical_path","text":"<pre><code>find_critical_path(n_steps=10, log_frequency=1000)\n</code></pre> <p>Find the critical path using gradient descent optimization.</p> <p>Args:     n_steps (int, optional): Number of optimization steps. Defaults to 10.     log_frequency (int, optional): Frequency of logging. Defaults to 1000.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent.py</code> <pre><code>def find_critical_path(\n        self,\n        n_steps: int = 10,\n        log_frequency: int = 1000\n) -&gt; None:\n    \"\"\"\n    Find the critical path using gradient descent optimization.\n\n    Args:\n        n_steps (int, optional): Number of optimization steps. Defaults to 10.\n        log_frequency (int, optional): Frequency of logging. Defaults to 1000.\n    \"\"\"\n    print(\"computing critical_path...\")\n    n_steps = n_steps if n_steps is not None else self.max_n_steps\n\n    for step in range(n_steps):\n        loss, grad = self.loss_fxn(self.integrator)\n        print(\"loss\", loss)\n        print(\"grad\", grad)\n        \"\"\"\n        self.path.params = update(\n            self.path.params,\n            self.grad_fxn,\n            self.metric_fxn,\n            self.learning_rate\n        )\n        \"\"\"\n        \"\"\"\n        if step % log_frequency == 0:\n            self.training_logger(\n                step,\n                self.action(self.potential, points, start, end)\n            )\n        \"\"\"\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent.html#chemistry_MEP_TS_optimization.optimization.gradient_descent.gradientDescent.update_path","title":"update_path","text":"<pre><code>update_path()\n</code></pre> <p>Update the path weights using gradient descent.</p> <p>Returns:     float: Loss value after the update.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef update_path(self) -&gt; float:\n    \"\"\"\n    Update the path weights using gradient descent.\n\n    Returns:\n        float: Loss value after the update.\n    \"\"\"\n    loss = self.loss_fxn(self.path, self.potential)\n    grads = jax.grad(loss)(self.path.weights)\n    self.path.weights = [(w - self.step_size*dw, b - self.step_size*db)\n                         for (w,b), (dw, db) in zip( self.path.weights, grads)]\n    \"\"\"\n    new_points = points -  self.path_step_factor*jax.grad(\n        self.action, argnums=1)(\n            self.potential,\n            points,\n            start,\n            end,\n        )\n    \"\"\"\n\n    return loss\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html","title":"gradient_descent_discrete","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent","title":"gradientDescent","text":"<pre><code>gradientDescent(potential, config, action, minima_step_factor=None, minima_num_steps=None, path_step_factor=None, path_num_steps=None)\n</code></pre> <p>               Bases: <code>logging</code></p> Parameters: <p>potential : callable     The potential function. config : dict     Configuration dictionary. action : callable     The action function. minima_step_factor : float, optional     The step factor for minima (default is None). minima_num_steps : int, optional     The number of steps for minima (default is None). path_step_factor : float, optional     The step factor for paths (default is None). path_num_steps : int, optional     The number of steps for paths (default is None).</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>def __init__(\n        self,\n        potential: callable,\n        config: dict,\n        action: callable,\n        minima_step_factor: float = None,\n        minima_num_steps: int = None,\n        path_step_factor: float = None,\n        path_num_steps: int = None,\n):\n    \"\"\"\n    Initialize the GradientDescent optimizer.\n\n    Parameters:\n    -----------\n    potential : callable\n        The potential function.\n    config : dict\n        Configuration dictionary.\n    action : callable\n        The action function.\n    minima_step_factor : float, optional\n        The step factor for minima (default is None).\n    minima_num_steps : int, optional\n        The number of steps for minima (default is None).\n    path_step_factor : float, optional\n        The step factor for paths (default is None).\n    path_num_steps : int, optional\n        The number of steps for paths (default is None).\n    \"\"\"\n    super().__init__()\n    self.potential = potential\n    self.config = config\n    self.action = action\n\n    if \"minima_step_factor\" in self.config and minima_step_factor is None:\n        self.minima_step_factor = self.config[\"minima_step_factor\"]\n    else:\n        assert minima_step_factor is not None\n        self.minima_step_factor = minima_step_factor\n    if \"minima_num_steps\" in self.config and minima_num_steps is None:\n        self.minima_num_steps = self.config[\"minima_num_steps\"]\n    else:\n        assert minima_num_steps is not None\n        self.minima_step_factor = minima_num_steps\n\n    if \"path_step_factor\" in self.config and path_step_factor is None:\n        self.path_step_factor = self.config[\"path_step_factor\"]\n    else:\n        assert path_step_factor is not None\n        self.minima_step_factor = path_step_factor\n    if \"path_num_steps\" in self.config and path_num_steps is None:\n        self.path_num_steps = self.config[\"path_num_steps\"]\n    else:\n        assert path_num_steps is not None\n        self.path_num_steps = path_num_steps\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent.find_critical_paths","title":"find_critical_paths","text":"<pre><code>find_critical_paths(initial_points, start, end, num_steps=None, log_frequency=1000)\n</code></pre> <p>Find critical paths.</p> Parameters: <p>initial_points : array_like     Initial points. start : array_like     Start point. end : array_like     End point. num_steps : int, optional     Number of steps (default is None). log_frequency : int, optional     Logging frequency (default is 1000).</p> Returns: <p>list     List of critical paths.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>def find_critical_paths(\n        self,\n        initial_points: jnp.ndarray,\n        start: jnp.ndarray,\n        end: jnp.ndarray,\n        num_steps: int = None,\n        log_frequency: int = 1000\n) -&gt; List[ jnp.ndarray]:\n    \"\"\"\n    Find critical paths.\n\n    Parameters:\n    -----------\n    initial_points : array_like\n        Initial points.\n    start : array_like\n        Start point.\n    end : array_like\n        End point.\n    num_steps : int, optional\n        Number of steps (default is None).\n    log_frequency : int, optional\n        Logging frequency (default is 1000).\n\n    Returns:\n    --------\n    list\n        List of critical paths.\n    \"\"\"\n    print(\"computing critical_path...\")\n    result = []\n    points = initial_points\n    result.append(points)\n    num_steps = num_steps if num_steps is not None else self.path_num_steps\n\n    for step in range(num_steps):\n        points = self.update_critical_path(points, start, end)\n        if step % log_frequency == 0:\n            result.append(points)\n            self.training_logger(\n                step,\n                self.action(self.potential, points, start, end)\n            )\n\n    result.append(points)\n\n    print(\"\\n\\n\\n\")\n    return result\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent.find_minima","title":"find_minima","text":"<pre><code>find_minima(initial_points=None)\n</code></pre> <p>Find the minima of the potential function.</p> Parameters: <p>initial_points : list, optional     List of initial points (default is None).</p> Returns: <p>list     List of minima points.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>def find_minima(\n        self,\n        initial_points=None) -&gt; list:\n    \"\"\"\n    Find the minima of the potential function.\n\n    Parameters:\n    -----------\n    initial_points : list, optional\n        List of initial points (default is None).\n\n    Returns:\n    --------\n    list\n        List of minima points.\n    \"\"\"\n    if \"minima\" in self.config:\n        initial_points = self.config[\"minima\"] if initial_points is None\\\n            else initial_points\n    else:\n        initial_points = [] if initial_points is None else initial_points\n\n    self.minima = [\n        self.find_minimum(jnp.array(point)) for point in initial_points\n    ]\n    return self.minima\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent.find_minimum","title":"find_minimum","text":"<pre><code>find_minimum(point, log_frequency=1000)\n</code></pre> <p>Find the minimum point.</p> Parameters: <p>point : jnp.ndarray     The initial point. log_frequency : int, optional     Logging frequency (default is 1000).</p> Returns: <p>jnp.ndarray     The minimum point.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>def find_minimum(\n        self,\n        point: jnp.ndarray,\n        log_frequency: int = 1000\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Find the minimum point.\n\n    Parameters:\n    -----------\n    point : jnp.ndarray\n        The initial point.\n    log_frequency : int, optional\n        Logging frequency (default is 1000).\n\n    Returns:\n    --------\n    jnp.ndarray\n        The minimum point.\n    \"\"\"\n    print(\"computing minima...\")\n\n    for step in range(self.minima_num_steps):\n        point = self.update_minimum(point)\n        if step % log_frequency == 0:\n            self.training_logger(step, self.potential(point))\n\n    return point\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent.update_critical_path","title":"update_critical_path","text":"<pre><code>update_critical_path(points, start, end)\n</code></pre> <p>Update critical path.</p> Parameters: <p>points : jnp.ndarray     Points. start : jnp.ndarray     Start point. end : jnp.ndarray     End point.</p> Returns: <p>jnp.ndarray     Updated points.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef update_critical_path(self,\n                         points: jnp.ndarray,\n                         start: jnp.ndarray,\n                         end: jnp.ndarray\n                         ) -&gt; jnp.ndarray:\n    \"\"\"\n    Update critical path.\n\n    Parameters:\n    -----------\n    points : jnp.ndarray\n        Points.\n    start : jnp.ndarray\n        Start point.\n    end : jnp.ndarray\n        End point.\n\n    Returns:\n    --------\n    jnp.ndarray\n        Updated points.\n    \"\"\"\n    new_points = points -  self.path_step_factor*jax.grad(\n        self.action, argnums=1)(\n            self.potential,\n            points,\n            start,\n            end,\n        )\n\n    return new_points\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_discrete.gradientDescent.update_minimum","title":"update_minimum","text":"<pre><code>update_minimum(point)\n</code></pre> <p>Update the point using gradient descent.</p> Parameters: <p>point : jnp.ndarray     The current point.</p> Returns: <p>jnp.ndarray     The updated point.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_discrete.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef update_minimum(self, point: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Update the point using gradient descent.\n\n    Parameters:\n    -----------\n    point : jnp.ndarray\n        The current point.\n\n    Returns:\n    --------\n    jnp.ndarray\n        The updated point.\n    \"\"\"\n    # returns the new point, and the val / grad norm at the old point.\n\n    grad = jax.grad(self.potential)(point)\n    new_point = point - self.minima_step_factor*grad\n\n    return new_point\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.html","title":"gradient_descent_orig","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_orig.gradientDescent_","title":"gradientDescent_","text":"<pre><code>gradientDescent_(potential, path, loss_fxn, metric_fxn, config, max_n_steps=1000000000.0)\n</code></pre> <p>               Bases: <code>ODEintegrator</code>, <code>logging</code></p> Parameters: <p>potential : callable     The potential function. path : object     The path object. loss_fxn : callable     The loss function. metric_fxn : callable     The metric function. config : dict     Configuration dictionary. max_n_steps : int, optional     Maximum number of steps (default is 1e9).</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.py</code> <pre><code>def __init__(\n        self,\n        potential: callable,\n        path: object,\n        loss_fxn: callable,\n        metric_fxn: callable,\n        config: dict,\n        max_n_steps: int = 1e9,\n):\n    \"\"\"\n    Initialize the gradient descent optimizer.\n\n    Parameters:\n    -----------\n    potential : callable\n        The potential function.\n    path : object\n        The path object.\n    loss_fxn : callable\n        The loss function.\n    metric_fxn : callable\n        The metric function.\n    config : dict\n        Configuration dictionary.\n    max_n_steps : int, optional\n        Maximum number of steps (default is 1e9).\n    \"\"\"\n    super().__init__(potential, path)\n    self.potential = potential\n    self.path\n    self.loss_fxn = loss_fxn\n    self.metric_fxn = metric_fxn\n    self.config = config\n    self.max_n_steps = max_n_steps\n    self.grad_fxn = jax.grad(self.loss_fxn)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_orig.gradientDescent_.find_critical_path","title":"find_critical_path","text":"<pre><code>find_critical_path(n_steps=10, log_frequency=1000)\n</code></pre> <p>Find the critical path.</p> Parameters: <p>n_steps : int, optional     Number of steps (default is 10). log_frequency : int, optional     Logging frequency (default is 1000).</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.py</code> <pre><code>def find_critical_path(\n        self,\n        n_steps: int = 10,\n        log_frequency: int = 1000\n) -&gt;  None:\n    \"\"\"\n    Find the critical path.\n\n    Parameters:\n    -----------\n    n_steps : int, optional\n        Number of steps (default is 10).\n    log_frequency : int, optional\n        Logging frequency (default is 1000).\n    \"\"\"\n    print(\"computing critical_path...\")\n    n_steps = n_steps if n_steps is not None else self.max_n_steps\n\n    for step in range(n_steps):\n        metrics = self.path_integral()\n        self.path.params = update(\n            self.path.params,\n            self.grad_fxn,\n            self.metric_fxn,\n            self.learning_rate\n        )\n        \"\"\"\n        if step % log_frequency == 0:\n            self.training_logger(\n                step,\n                self.action(self.potential, points, start, end)\n            )\n        \"\"\"\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_orig.gradientDescent_.update_path","title":"update_path","text":"<pre><code>update_path()\n</code></pre> <p>Update the path.</p> Returns: <p>float     Loss value.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.py</code> <pre><code>@partial(jax.jit, static_argnums=[0])\ndef update_path(self) -&gt; float:\n    \"\"\"\n    Update the path.\n\n    Returns:\n    --------\n    float\n        Loss value.\n    \"\"\"\n    loss = self.loss_fxn(self.path, self.potential)\n    grads = jax.grad(loss)(self.path.weights)\n    self.path.weights = [(w - self.step_size*dw, b - self.step_size*db)\\\n        for (w,b), (dw, db) in zip( self.path.weights, grads)]\n    \"\"\"\n    new_points = points -  self.path_step_factor*jax.grad(\n        self.action, argnums=1)(\n            self.potential,\n            points,\n            start,\n            end,\n        )\n    \"\"\"\n\n    return loss\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.html#chemistry_MEP_TS_optimization.optimization.gradient_descent_orig.update","title":"update","text":"<pre><code>update(params, grad_fxn, metrics, learning_rate)\n</code></pre> <p>Update parameters using gradient descent.</p> Parameters: <p>params : dict     Model parameters. grad_fxn : callable     Gradient function. metrics : dict     Metrics. learning_rate : float     Learning rate.</p> Returns: <p>dict     Updated parameters.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/gradient_descent_orig.py</code> <pre><code>@jax.jit\ndef update(\n        params: dict,\n        grad_fxn: callable,\n        metrics: dict,\n        learning_rate: float\n) -&gt; dict:\n    \"\"\"\n    Update parameters using gradient descent.\n\n    Parameters:\n    -----------\n    params : dict\n        Model parameters.\n    grad_fxn : callable\n        Gradient function.\n    metrics : dict\n        Metrics.\n    learning_rate : float\n        Learning rate.\n\n    Returns:\n    --------\n    dict\n        Updated parameters.\n    \"\"\"\n    grads = grad_fxn(params, metrics)\n    return jax.tree_map(\n        lambda param, g: param - g*learning_rate, params, grads\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/initialize_path.html","title":"initialize_path","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/initialize_path.html#chemistry_MEP_TS_optimization.optimization.initialize_path.initialize_path","title":"initialize_path","text":"<pre><code>initialize_path(path, times, init_points, lr=0.001, max_steps=5000)\n</code></pre> <p>Initialize the path.</p> Parameters: <p>path : torch.Tensor     The path object. times : torch.Tensor     Times. init_points : torch.Tensor     Initial points. lr : float, optional     Learning rate (default is 0.001). max_steps : int, optional     Maximum number of steps (default is 5000).</p> Returns: <p>torch.Tensor     Initialized path.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/initialize_path.py</code> <pre><code>def initialize_path(\n        path: torch.tensor,\n        times: torch.tensor,\n        init_points: torch.tensor,\n        lr: float = 0.001,\n        max_steps: int = 5000\n) -&gt; torch.tensor:\n    \"\"\"\n    Initialize the path.\n\n    Parameters:\n    -----------\n    path : torch.Tensor\n        The path object.\n    times : torch.Tensor\n        Times.\n    init_points : torch.Tensor\n        Initial points.\n    lr : float, optional\n        Learning rate (default is 0.001).\n    max_steps : int, optional\n        Maximum number of steps (default is 5000).\n\n    Returns:\n    --------\n    torch.Tensor\n        Initialized path.\n    \"\"\"\n    print(\"INFO: Beginning path initialization\")\n    loss, prev_loss = torch.tensor([2e-10]), torch.tensor([1e-10])\n    print(path.named_parameters())\n    optimizer = torch.optim.Adam(path.parameters(), lr=lr)\n    idx, rel_error = 0, 100\n    while (idx &lt; 1500 or loss &gt; 1e-8) and idx &lt; max_steps:\n        optimizer.zero_grad()\n\n        prev_loss = loss.item()\n        loss = loss_init(path, times, init_points)\n\n        loss.backward()\n        optimizer.step()\n        rel_error = np.abs(prev_loss - loss.item())/prev_loss\n        idx = idx + 1\n        if idx % 250 == 0:\n            print(f\"\\tIteration {idx}: Loss {loss:.4} | Relative Error {rel_error:.5}\")\n            fig, ax = plt.subplots()\n            path_output = path.get_path()\n            geometric_path = path_output.geometric_path.detach().numpy()\n            ax.plot(init_points[:,0], init_points[:,1], 'ob')\n            ax.plot(geometric_path[:,0], geometric_path[:,1], '-k')\n            fig.savefig(f\"./plots/initialization/init_path_{idx}.png\")\n\n        #print(prev_loss, loss, jnp.abs(prev_loss - loss)/prev_loss)\n\n    print(f\"INFO: Finished path initialization after {idx} iterations\")\n    fig, ax = plt.subplots()\n    path_output = path.get_path()\n    geometric_path = path_output.geometric_path.detach().numpy()\n    ax.plot(init_points[:,0], init_points[:,1], 'ob')\n    ax.plot(geometric_path[:,0], geometric_path[:,1], '-k')\n    fig.savefig(\"./plots/init_path.png\")\n\n    return path\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/initialize_path.html#chemistry_MEP_TS_optimization.optimization.initialize_path.loss_init","title":"loss_init","text":"<pre><code>loss_init(path, times, points)\n</code></pre> <p>Initialize the loss.</p> Parameters: <p>path : torch.Tensor     The path object. times : torch.Tensor     Times. points : torch.Tensor     Points.</p> Returns: <p>torch.Tensor     Loss value.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/initialize_path.py</code> <pre><code>def loss_init(\n        path: torch.tensor,\n        times: torch.tensor,\n        points: torch.tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Initialize the loss.\n\n    Parameters:\n    -----------\n    path : torch.Tensor\n        The path object.\n    times : torch.Tensor\n        Times.\n    points : torch.Tensor\n        Points.\n\n    Returns:\n    --------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    preds = path.geometric_path(times)\n    return torch.mean((points - preds)**2)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/initialize_path.html#chemistry_MEP_TS_optimization.optimization.initialize_path.randomly_initialize_path","title":"randomly_initialize_path","text":"<pre><code>randomly_initialize_path(path, n_points, order_points=False, seed=1910)\n</code></pre> <p>Randomly initialize the path.</p> Parameters: <p>path : torch.Tensor     The path object. n_points : int     Number of points. order_points : bool, optional     Whether to order points (default is False). seed : int, optional     Random seed (default is 1910).</p> Returns: <p>Union[torch.Tensor, np.ndarray]     Initialized path.</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/initialize_path.py</code> <pre><code>def randomly_initialize_path(\n        path: torch.tensor,\n        n_points: int,\n        order_points: bool = False,\n        seed: int = 1910\n) -&gt; Union[torch.Tensor, np.ndarray]:\n    \"\"\"\n    Randomly initialize the path.\n\n    Parameters:\n    -----------\n    path : torch.Tensor\n        The path object.\n    n_points : int\n        Number of points.\n    order_points : bool, optional\n        Whether to order points (default is False).\n    seed : int, optional\n        Random seed (default is 1910).\n\n    Returns:\n    --------\n    Union[torch.Tensor, np.ndarray]\n        Initialized path.\n    \"\"\"\n    #times = rnd.uniform(shape=(n_points, 1), minval=0.1, maxval=0.9)\n    times = torch.unsqueeze(torch.linspace(0, 1, n_points+2)[1:-1], -1)\n    times.requires_grad = False\n\n    n_dims = len(path.initial_point)\n    rnd_dims = []\n    for idx in range(n_dims):\n        min_val = torch.min(\n            torch.tensor([path.initial_point[idx], path.final_point[idx]])\n        ).item()\n        max_val = torch.max(\n            torch.tensor([path.initial_point[idx], path.final_point[idx]])\n        ).item()\n        print(\"MIN MAX\", min_val, max_val)\n        rnd_vals = rnd.uniform(size=(n_points, 1), low=min_val, high=max_val)\n        if order_points or idx == 0:\n            if path.initial_point[idx] &gt; path.final_point[idx]:\n                rnd_dims.append(-1*np.sort(-1*rnd_vals, axis=0))\n            else:\n                rnd_dims.append(np.sort(rnd_vals, axis=0))\n        else:\n            rnd_dims.append(rnd_vals)\n    print(len(rnd_dims), rnd_dims[0].shape)\n    rnd_dims = torch.tensor(\n        np.concatenate(rnd_dims, axis=-1), requires_grad=False\n    )\n\n    return initialize_path(path, times, rnd_dims)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/losses.html","title":"losses","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/path_metrics_orig.html","title":"path_metrics_orig","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/path_optimizer.html","title":"path_optimizer","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/update_minima.html","title":"update_minima","text":""},{"location":"reference/chemistry_MEP_TS_optimization/optimization/update_minima.html#chemistry_MEP_TS_optimization.optimization.update_minima.MinimaUpdate","title":"MinimaUpdate","text":"<pre><code>MinimaUpdate(potential, n_steps=10000, step_size=0.01)\n</code></pre> Source code in <code>chemistry_MEP_TS_optimization/optimization/update_minima.py</code> <pre><code>def __init__(self, potential, n_steps=10000, step_size=1e-2):\n    self.potential = potential\n    self.step_size = step_size\n    self.n_steps = n_steps\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/optimization/update_minima.html#chemistry_MEP_TS_optimization.optimization.update_minima.MinimaUpdate.find_minimum","title":"find_minimum","text":"<pre><code>find_minimum(point, log_frequency=1000)\n</code></pre> <p>loop for finding minima</p> Source code in <code>chemistry_MEP_TS_optimization/optimization/update_minima.py</code> <pre><code>def find_minimum(self, point, log_frequency=1000):\n    \"\"\"\n    loop for finding minima\n    \"\"\"\n    # Adding batch dimension if point is a single point\n    unsqueeze = False\n    if len(point.shape) == 1:\n        point = point.unsqueeze(0)\n        unsqueeze = True\n\n    #point = torch.nn.Parameter(point, requires_grad=True)\n    point.requires_grad = True\n    optimizer = torch.optim.SGD([point], lr=self.step_size)\n    print(f\"computing minima ... {point}\")\n    for step in range(self.n_steps):\n        energy = torch.sum(self.potential(point))\n        energy.backward()\n        optimizer.step()\n        #if step % log_frequency == 0:\n        #    self.training_logger(step, self.potential(point))\n    point.requires_grad = False\n\n    if unsqueeze:\n        point = point[0]   \n    return point\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/b_spline.html","title":"b_spline","text":""},{"location":"reference/chemistry_MEP_TS_optimization/paths/b_spline.html#chemistry_MEP_TS_optimization.paths.b_spline.BSpline","title":"BSpline","text":"<pre><code>BSpline(potential, initial_point, final_point, degree=2, n_anchors=4, **kwargs)\n</code></pre> <p>               Bases: <code>BasePath</code></p> <p>B-Spline path representation.</p> Attributes: <p>degree : int     The degree of the B-Spline. points : jnp.array     The control points of the B-Spline. knots : jnp.array     The knot vector of the B-Spline.</p> Parameters: <p>potential : A callable function     The potential function. initial_point : jnp.array     The initial point of the path. final_point : jnp.array     The final point of the path. degree : int, optional     The degree of the B-Spline (default is 2). n_anchors : int, optional     The number of anchor points (default is 4). **kwargs : Any     Additional keyword arguments.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/b_spline.py</code> <pre><code>def __init__(\n    self,\n    potential: callable,\n    initial_point: jnp.array,\n    final_point: jnp.array,\n    degree: int = 2,\n    n_anchors: int = 4,\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize the B-Spline path.\n\n    Parameters:\n    -----------\n    potential : A callable function\n        The potential function.\n    initial_point : jnp.array\n        The initial point of the path.\n    final_point : jnp.array\n        The final point of the path.\n    degree : int, optional\n        The degree of the B-Spline (default is 2).\n    n_anchors : int, optional\n        The number of anchor points (default is 4).\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        potential=potential,\n        initial_point=initial_point,\n        final_point=final_point,\n        **kwargs\n    )\n\n    self.degree = degree\n    delta_geo = (self.final_point - self.initial_point)/float(n_anchors + 2) \n    self.points = jnp.array([\n        self.initial_point + delta_geo*(i + 1) for i in range(n_anchors)\n    ])\n    delta_time = 1./(n_anchors + 1)\n    self.knots = jnp.array([\n        (i + 1)/float(n_anchors + 1) for i in range(n_anchors)\n    ])\n    print(\"This method is not finished\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/b_spline.html#chemistry_MEP_TS_optimization.paths.b_spline.BSpline.geometric_path","title":"geometric_path","text":"<pre><code>geometric_path(time, y, *args)\n</code></pre> <p>Compute the geometric path at the given time.</p> Parameters: <p>time : float     The time at which to evaluate the geometric path. y : Any     Placeholder for additional arguments. *args : Any     Additional arguments.</p> Returns: <p>None</p> Source code in <code>chemistry_MEP_TS_optimization/paths/b_spline.py</code> <pre><code>def geometric_path(\n        self,\n        time: float,\n        y: Any,\n        *args: Any\n) -&gt; None:\n    \"\"\"\n    Compute the geometric path at the given time.\n\n    Parameters:\n    -----------\n    time : float\n        The time at which to evaluate the geometric path.\n    y : Any\n        Placeholder for additional arguments.\n    *args : Any\n        Additional arguments.\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    idx = self.degree + int(time/self.delta_time)\n    time_diffs = time - self.knots[idx-(self.degree-1):idx+self.degree]\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html","title":"base_path","text":""},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html#chemistry_MEP_TS_optimization.paths.base_path.BasePath","title":"BasePath","text":"<pre><code>BasePath(potential, initial_point, final_point, return_velocity=False, return_force=False, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for path representation.</p> Attributes: <p>initial_point : torch.Tensor     The initial point of the path. final_point : torch.Tensor     The final point of the path. potential : PotentialBase     The potential function.</p> Methods: <p>geometric_path(time, y, *args) -&gt; torch.Tensor:     Compute the geometric path at the given time.</p> <p>get_path(times=None, return_velocity=False, return_force=False) -&gt; PathOutput:     Get the path for the given times.</p> <p>forward(t, return_velocity=False, return_force=False) -&gt; PathOutput:     Compute the path output for the given times.</p> Parameters: <p>potential : callable     The potential function. initial_point : torch.Tensor     The initial point of the path. final_point : torch.Tensor     The final point of the path. return_velocity : bool, optional     Whether to return velocity along the path (default is False). return_force : bool, optional     Whether to return force along the path (default is False). **kwargs : Any     Additional keyword arguments.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/base_path.py</code> <pre><code>def __init__(\n    self,\n    potential: Callable,\n    initial_point: torch.Tensor,\n    final_point: torch.Tensor,\n    return_velocity: bool = False,\n    return_force: bool = False,\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize the BasePath.\n\n    Parameters:\n    -----------\n    potential : callable\n        The potential function.\n    initial_point : torch.Tensor\n        The initial point of the path.\n    final_point : torch.Tensor\n        The final point of the path.\n    return_velocity : bool, optional\n        Whether to return velocity along the path (default is False).\n    return_force : bool, optional\n        Whether to return force along the path (default is False).\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n    super().__init__()\n    self.potential = potential\n    self.initial_point = torch.tensor(initial_point)\n    self.final_point = torch.tensor(final_point)\n    self.return_velocity = return_velocity\n    self.return_force = return_force\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html#chemistry_MEP_TS_optimization.paths.base_path.BasePath.forward","title":"forward","text":"<pre><code>forward(t, return_velocity=False, return_force=False)\n</code></pre> <p>Forward pass to compute the path, potential, velocity, and force.</p> Parameters: <p>t : torch.Tensor     The time tensor at which to evaluate the path. return_velocity : bool, optional     Whether to return velocity along the path (default is False). return_force : bool, optional     Whether to return force along the path (default is False).</p> Returns: <p>PathOutput     An instance of the PathOutput class containing the computed path, potential, velocity, force, and times.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/base_path.py</code> <pre><code>def forward(\n        self,\n        t,\n        return_velocity: bool = False,\n        return_force: bool = False\n) -&gt; PathOutput:\n    \"\"\"\n    Forward pass to compute the path, potential, velocity, and force.\n\n    Parameters:\n    -----------\n    t : torch.Tensor\n        The time tensor at which to evaluate the path.\n    return_velocity : bool, optional\n        Whether to return velocity along the path (default is False).\n    return_force : bool, optional\n        Whether to return force along the path (default is False).\n\n    Returns:\n    --------\n    PathOutput\n        An instance of the PathOutput class containing the computed path, potential, velocity, force, and times.\n    \"\"\"\n    geo_path = self.geometric_path(t)\n    pes_path = self.potential(geo_path)\n\n    velocity, force = None, None\n    is_batched = len(pes_path.shape) &gt; 0\n    if self.return_force or return_force:\n        #print(\"SHAPES\", pes_path.shape, len(pes_path.shape), torch.ones(0), geo_path.shape)\n        #print(\"CHECK IS GRADS BATCHD FOR LEN &gt; 0\")\n        force = torch.autograd.grad(\n            torch.sum(pes_path),\n            geo_path,\n            create_graph=(not is_batched),\n        )[0]\n        #print(\"LEN F\", len(force), force[0].shape)\n        if not is_batched:\n            force = torch.unsqueeze(force, 0)\n        #print(\"FORCES\", force.shape)\n    if self.return_velocity or return_velocity:\n        #print(\"VEL SHAPES\", geo_path.shape, t.shape)\n        if is_batched:\n            fxn = lambda t: torch.sum(self.geometric_path(t), axis=0)\n        else:\n            fxn = lambda t: self.geometric_path(t)\n        velocity = torch.autograd.functional.jacobian(\n            fxn, t, create_graph=(not is_batched), vectorize=is_batched\n        )\n        #print(\"VEL INIT SHAPE\", velocity.shape)\n        #print(\"VEL TEST\", velocity[:5])\n        velocity = torch.transpose(velocity, 0, 1)\n        if is_batched:\n            velocity = velocity[:,:,0]\n        #print(\"VEL F OUTPUT\", velocity.shape, force.shape)\n\n    return PathOutput(\n        geometric_path=geo_path,\n        potential_path=pes_path,\n        velocity=velocity,\n        force=force,\n        times=t\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html#chemistry_MEP_TS_optimization.paths.base_path.BasePath.geometric_path","title":"geometric_path","text":"<pre><code>geometric_path(time, y, *args)\n</code></pre> <p>Compute the geometric path at the given time.</p> Parameters: <p>time : torch.Tensor     The time at which to evaluate the geometric path. y : Any     Placeholder for additional arguments. *args : Any     Additional arguments.</p> Returns: <p>torch.Tensor     The geometric path at the given time.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/base_path.py</code> <pre><code>def geometric_path(\n        self,\n        time: torch.Tensor,\n        y: Any,\n        *args: Any\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the geometric path at the given time.\n\n    Parameters:\n    -----------\n    time : torch.Tensor\n        The time at which to evaluate the geometric path.\n    y : Any\n        Placeholder for additional arguments.\n    *args : Any\n        Additional arguments.\n\n    Returns:\n    --------\n    torch.Tensor\n        The geometric path at the given time.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html#chemistry_MEP_TS_optimization.paths.base_path.BasePath.get_path","title":"get_path","text":"<pre><code>get_path(times=None, return_velocity=False, return_force=False)\n</code></pre> <p>Get the path for the given times.</p> Parameters: <p>times : torch.Tensor, optional     The times at which to evaluate the path (default is None). return_velocity : bool, optional     Whether to return velocity along the path (default is False). return_force : bool, optional     Whether to return force along the path (default is False).</p> Returns: <p>PathOutput     An instance of the PathOutput class representing the computed path.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/base_path.py</code> <pre><code>def get_path(\n        self,\n        times: torch.Tensor = None,\n        return_velocity: bool = False,\n        return_force: bool = False\n) -&gt; PathOutput:\n    \"\"\"\n    Get the path for the given times.\n\n    Parameters:\n    -----------\n    times : torch.Tensor, optional\n        The times at which to evaluate the path (default is None).\n    return_velocity : bool, optional\n        Whether to return velocity along the path (default is False).\n    return_force : bool, optional\n        Whether to return force along the path (default is False).\n\n    Returns:\n    --------\n    PathOutput\n        An instance of the PathOutput class representing the computed path.\n    \"\"\"\n    if times is None:\n        times = torch.unsqueeze(torch.linspace(0, 1., 1000), -1)\n    elif len(times.shape) == 1:\n        times = torch.unsqueeze(times, -1)\n\n    return self.forward(\n        times, return_velocity=return_velocity, return_force=return_force\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/base_path.html#chemistry_MEP_TS_optimization.paths.base_path.PathOutput","title":"PathOutput  <code>dataclass</code>","text":"<pre><code>PathOutput(geometric_path, potential_path, velocity=None, force=None, times=None)\n</code></pre> <p>Data class representing the output of a path computation.</p> Attributes: <p>geometric_path : torch.Tensor     The geometric path. potential_path : torch.Tensor     The potential path. velocity : torch.Tensor, optional     The velocity along the path (default is None). force : torch.Tensor, optional     The force along the path (default is None). times : torch.Tensor, optional     The times at which the path was evaluated (default is None).</p>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/elastic_band.html","title":"elastic_band","text":""},{"location":"reference/chemistry_MEP_TS_optimization/paths/elastic_band.html#chemistry_MEP_TS_optimization.paths.elastic_band.ElasticBand","title":"ElasticBand","text":"<pre><code>ElasticBand(initial_point, final_point, n_images=50, special_point=None)\n</code></pre> <p>Elastic Band class representing a collection of images between two points.</p> Attributes: <p>initial_point : jnp.array     The initial point of the elastic band. final_point : jnp.array     The final point of the elastic band. special_point : jnp.array     The special point around which the elastic band is formed. n_images : int     The number of images in the elastic band. path : jnp.array     The computed path containing the images.</p> Methods: <p>compute_initial_points(start, end, n_images) -&gt; jnp.array:     Compute the initial points between start and end.</p> Parameters: <p>initial_point : np.ndarray     The initial point of the elastic band. final_point : np.ndarray     The final point of the elastic band. n_images : int, optional     The number of images in the elastic band (default is 50). special_point : np.ndarray, optional     The special point around which the elastic band is formed     (default is None, in which case it is the midpoint between initial_point and final_point).</p> Source code in <code>chemistry_MEP_TS_optimization/paths/elastic_band.py</code> <pre><code>def __init__(\n        self,\n        initial_point: np.ndarray,\n        final_point: np.ndarray,\n        n_images: int = 50,\n        special_point: np.ndarray = None\n    ) -&gt; None:\n    \"\"\"\n    Initialize the ElasticBand instance.\n\n    Parameters:\n    -----------\n    initial_point : np.ndarray\n        The initial point of the elastic band.\n    final_point : np.ndarray\n        The final point of the elastic band.\n    n_images : int, optional\n        The number of images in the elastic band (default is 50).\n    special_point : np.ndarray, optional\n        The special point around which the elastic band is formed\n        (default is None, in which case it is the midpoint between initial_point and final_point).\n    \"\"\"\n    self.initial_point = jnp.array(initial_point)\n    self.final_point = jnp.array(final_point)\n    if special_point is None:\n        self.special_point = jnp.array(initial_point) + jnp.array(final_point)\n        self.special_point = self.special_point/2\n    else:\n        self.special_point = jnp.array(special_point)\n    self.n_images = n_images\n\n    images_1 = self.compute_initial_points(\n        self.initial_point, self.special_point, self.n_images//2\n    )\n    images_2 = self.compute_initial_points(\n        self.special_point, self.final_point, self.n_images//2\n    )\n    self.path = jnp.vstack([images_1, images_2])\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/elastic_band.html#chemistry_MEP_TS_optimization.paths.elastic_band.ElasticBand.compute_initial_points","title":"compute_initial_points","text":"<pre><code>compute_initial_points(start, end, n_images)\n</code></pre> <p>Compute the initial points between start and end.</p> Parameters: <p>start : jnp.array     The start point. end : jnp.array     The end point. n_images : int     The number of images to compute.</p> Returns: <p>jnp.array     The computed initial points.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/elastic_band.py</code> <pre><code>def compute_initial_points(\n        self,\n        start: jnp.array,\n        end: jnp.array,\n        n_images: int\n) -&gt; jnp.array:\n    \"\"\"\n    Compute the initial points between start and end.\n\n    Parameters:\n    -----------\n    start : jnp.array\n        The start point.\n    end : jnp.array\n        The end point.\n    n_images : int\n        The number of images to compute.\n\n    Returns:\n    --------\n    jnp.array\n        The computed initial points.\n    \"\"\"\n    ts = np.linspace(0.0, 1.0, n_images+1)[1:]\n    points = [start*(1 - t) + end*t for t in ts]\n    return jnp.stack(points)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/initialize.html","title":"initialize","text":""},{"location":"reference/chemistry_MEP_TS_optimization/paths/initialize.html#chemistry_MEP_TS_optimization.paths.initialize.random_layer_params","title":"random_layer_params","text":"<pre><code>random_layer_params(m, n, key, scale=1)\n</code></pre> <p>Generate random weights and biases for a neural network layer.</p> Parameters: <p>m : int     Number of input neurons. n : int     Number of output neurons. key : jax.random.PRNGKey     The random key for generating the parameters. scale : float, optional     Scaling factor for the random parameters (default is 1).</p> Returns: <p>Tuple[jnp.ndarray, jnp.ndarray]     Tuple containing the randomly initialized weights and biases.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/initialize.py</code> <pre><code>def random_layer_params(\n        m: int,\n        n: int,\n        key: jax.random.PRNGKey,\n        scale: float = 1\n) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Generate random weights and biases for a neural network layer.\n\n    Parameters:\n    -----------\n    m : int\n        Number of input neurons.\n    n : int\n        Number of output neurons.\n    key : jax.random.PRNGKey\n        The random key for generating the parameters.\n    scale : float, optional\n        Scaling factor for the random parameters (default is 1).\n\n    Returns:\n    --------\n    Tuple[jnp.ndarray, jnp.ndarray]\n        Tuple containing the randomly initialized weights and biases.\n    \"\"\"\n    w_key, b_key = random.split(key)\n    return scale*random.normal(w_key, (n, m)), scale*random.normal(b_key, (m,))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/mlp.html","title":"mlp","text":""},{"location":"reference/chemistry_MEP_TS_optimization/paths/mlp.html#chemistry_MEP_TS_optimization.paths.mlp.MLPpath","title":"MLPpath","text":"<pre><code>MLPpath(potential, initial_point, final_point, n_embed=32, depth=3, seed=123)\n</code></pre> <p>               Bases: <code>BasePath</code></p> <p>Multilayer Perceptron (MLP) path class for generating geometric paths.</p> <p>Args:     potential (callable): The potential energy function.     initial_point (torch.Tensor): The initial point of the path.     final_point (torch.Tensor): The final point of the path.     n_embed (int, optional): Number of embedding dimensions. Defaults to 32.     depth (int, optional): Depth of the MLP. Defaults to 3.     seed (int, optional): Random seed. Defaults to 123.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/mlp.py</code> <pre><code>def __init__(\n    self,\n    potential: callable,\n    initial_point: torch.tensor,\n    final_point: torch.tensor,\n    n_embed: int = 32,\n    depth: int = 3,\n    seed: int = 123,\n):\n    super().__init__(\n        potential=potential,\n        initial_point=initial_point,\n        final_point=final_point,\n    )\n    self.activation = nn.SELU()\n    input_sizes = [1] + [n_embed]*(depth - 1)\n    output_sizes = input_sizes[1:] + [self.final_point.shape[-1]]\n    self.layers = [\n        nn.Linear(input_sizes[i//2], output_sizes[i//2]) if i%2 == 0\\\n        else self.activation\\\n        for i in range(depth*2 - 1)\n    ]\n    self.mlp = nn.Sequential(*self.layers)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/paths/mlp.html#chemistry_MEP_TS_optimization.paths.mlp.MLPpath.geometric_path","title":"geometric_path","text":"<pre><code>geometric_path(time, *args)\n</code></pre> <p>Generates a geometric path using the MLP.</p> <p>Args:     time (float): Time parameter for generating the path.     *args: Additional arguments.</p> <p>Returns:     torch.Tensor: The geometric path generated by the MLP.</p> Source code in <code>chemistry_MEP_TS_optimization/paths/mlp.py</code> <pre><code>def geometric_path(self, time: float, *args):\n    \"\"\"\n    Generates a geometric path using the MLP.\n\n    Args:\n        time (float): Time parameter for generating the path.\n        *args: Additional arguments.\n\n    Returns:\n        torch.Tensor: The geometric path generated by the MLP.\n    \"\"\"\n    return self.mlp(time)\\\n        - (1 - time)*(self.mlp(torch.tensor([0.])) - self.initial_point)\\\n        - time*(self.mlp(torch.tensor([1.])) - self.final_point)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html","title":"eqx_example","text":""},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.Func","title":"Func","text":"<pre><code>Func(data_size, out_size, width_size, depth, *, key, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Function class using an MLP neural network.</p> <p>Args:     data_size (int): Input size.     out_size (int): Output size.     width_size (int): Width of the MLP.     depth (int): Depth of the MLP.     key (jax.random.PRNGKey): Random key for initialization.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __init__(\n        self,\n        data_size: int,\n        out_size: int,\n        width_size: int,\n        depth: int,\n        *,\n        key: jrandom.PRNGKey,\n        **kwargs\n):\n    super().__init__(**kwargs)\n    self.mlp = eqx.nn.MLP(\n        in_size=data_size,\n        out_size=out_size,\n        width_size=width_size,\n        depth=depth,\n        activation=jnn.softplus,\n        key=key,\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.Func.__call__","title":"__call__","text":"<pre><code>__call__(t, y, args)\n</code></pre> <p>Compute the function output.</p> <p>Args:     t (jnp.array): Time.     y (jnp.array): Input data.     args: Additional arguments.</p> <p>Returns:     jnp.array: Output of the function.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __call__(\n        self,\n        t: jnp.array,\n        y: jnp.array,\n        args\n) -&gt; jnp.array:\n    \"\"\"\n    Compute the function output.\n\n    Args:\n        t (jnp.array): Time.\n        y (jnp.array): Input data.\n        args: Additional arguments.\n\n    Returns:\n        jnp.array: Output of the function.\n    \"\"\"\n    return self.mlp(jnp.array([t]))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.NeuralODE","title":"NeuralODE","text":"<pre><code>NeuralODE(data_size, out_size, width_size, depth, *, key, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Neural Ordinary Differential Equation (ODE) class.</p> <p>Args:     data_size (int): Input size.     out_size (int): Output size.     width_size (int): Width of the MLP.     depth (int): Depth of the MLP.     key (jax.random.PRNGKey): Random key for initialization.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __init__(\n        self,\n        data_size: int,\n        out_size: int,\n        width_size: int,\n        depth: int,\n        *,\n        key: jrandom.PRNGKey,\n        **kwargs\n):\n    super().__init__(**kwargs)\n    if is_test:\n        self.func = TestFunc(data_size, out_size, width_size, depth, key=key)\n    else:\n        self.func = Func(data_size, out_size, width_size, depth, key=key)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.NeuralODE.__call__","title":"__call__","text":"<pre><code>__call__(ts, y0)\n</code></pre> <p>Solve the differential equation.</p> <p>Args:     ts (jnp.array): Time points.     y0 (jnp.array): Initial value.</p> <p>Returns:     jnp.array: Solution of the differential equation.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __call__(\n        self,\n        ts: jnp.array,\n        y0: jnp.array\n) -&gt; jnp.array:\n    \"\"\"\n    Solve the differential equation.\n\n    Args:\n        ts (jnp.array): Time points.\n        y0 (jnp.array): Initial value.\n\n    Returns:\n        jnp.array: Solution of the differential equation.\n    \"\"\"\n    solution = diffrax.diffeqsolve(\n        diffrax.ODETerm(self.func),\n        diffrax.Tsit5(),\n        t0=ts[0],\n        t1=ts[-1],\n        dt0=ts[1] - ts[0],\n        y0=y0,\n        stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n        saveat=diffrax.SaveAt(ts=ts),\n    )\n    return solution.ys\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.TestFunc","title":"TestFunc","text":"<pre><code>TestFunc(data_size, out_size, width_size, depth, *, key, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Test function class using a predefined weight matrix.</p> <p>Args:     data_size (int): Input size.     out_size (int): Output size.     width_size (int): Width of the MLP.     depth (int): Depth of the MLP.     key (jax.random.PRNGKey): Random key for initialization.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __init__(self, data_size, out_size, width_size, depth, *, key, **kwargs):\n    super().__init__(**kwargs)\n    self.weight = jnp.ones((data_size, out_size))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.TestFunc.__call__","title":"__call__","text":"<pre><code>__call__(t, y, args)\n</code></pre> <p>Compute the function output.</p> <p>Args:     t (jnp.array): Time.     y (jnp.array): Input data.     args: Additional arguments.</p> <p>Returns:     jnp.array: Output of the function.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>def __call__(\n        self,\n        t: jnp.array,\n        y: jnp.array,\n        args\n) -&gt; jnp.array:\n    \"\"\"\n    Compute the function output.\n\n    Args:\n        t (jnp.array): Time.\n        y (jnp.array): Input data.\n        args: Additional arguments.\n\n    Returns:\n        jnp.array: Output of the function.\n    \"\"\"\n    return jnp.matmul(jnp.array([t]), self.weight)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/eqx_example.html#chemistry_MEP_TS_optimization.playground.eqx_example.grad_loss","title":"grad_loss","text":"<pre><code>grad_loss(model, ti, yi)\n</code></pre> <p>Compute the loss and gradients of the loss with respect to the model's parameters.</p> <p>Args:     model (NeuralODE): NeuralODE model.     ti (jnp.array): Time points.     yi (jnp.array): Input data.</p> <p>Returns:     jnp.array: Loss value.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/eqx_example.py</code> <pre><code>@eqx.filter_value_and_grad\ndef grad_loss(\n        model: NeuralODE,\n        ti: jnp.array,\n        yi: jnp.array\n):\n    \"\"\"\n    Compute the loss and gradients of the loss with respect to the model's parameters.\n\n    Args:\n        model (NeuralODE): NeuralODE model.\n        ti (jnp.array): Time points.\n        yi (jnp.array): Input data.\n\n    Returns:\n        jnp.array: Loss value.\n    \"\"\"\n    # y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n    y_pred = model(ti, jnp.array([0,0]))\n    if is_test:\n        return jnp.linalg.norm(y_pred) + jnp.linalg.norm(model.func.weight)\n    else:\n        return jnp.linalg.norm(y_pred) + jnp.linalg.norm(model.func.mlp.layers[0].weight)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html","title":"example","text":""},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.path","title":"path  <code>module-attribute</code>","text":"<pre><code>path = Func(1, 2, 4, 2)\n</code></pre> <p>class path(eqx.nn.Module):     anything after works     weights: array</p> <pre><code>def _init__():\ndef __call__():\n    calculate spline/mlp\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.Func","title":"Func","text":"<pre><code>Func(data_size, width_size, depth, *, key, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Function class using an MLP neural network.</p> <p>Args:     data_size (int): Input size.     width_size (int): Width of the MLP.     depth (int): Depth of the MLP.     key (jax.random.PRNGKey): Random key for initialization.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def __init__(\n        self,\n        data_size: int,\n        width_size: int,\n        depth: int,\n        *,\n        key: jax.random.PRNGKey,\n        **kwargs\n):\n    super().__init__(**kwargs)\n    self.mlp = eqx.nn.MLP(\n        in_size=data_size,\n        out_size=data_size,\n        width_size=width_size,\n        depth=depth,\n        activation=jnn.softplus,\n        key=key,\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.Func.__call__","title":"__call__","text":"<pre><code>__call__(t, y, args)\n</code></pre> <p>Compute the function output.</p> <p>Args:     t (jnp.array): Time.     y (jnp.array): Input data.     args (float): Additional argument.</p> <p>Returns:     jnp.array: Output of the function.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def __call__(\n        self,\n        t: jnp.array,\n        y: jnp.array,\n        args\n) -&gt; jnp.array:\n    \"\"\"\n    Compute the function output.\n\n    Args:\n        t (jnp.array): Time.\n        y (jnp.array): Input data.\n        args (float): Additional argument.\n\n    Returns:\n        jnp.array: Output of the function.\n    \"\"\"\n    return jnp.linalg.norm(self.mlp(t))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.int_fxn","title":"int_fxn","text":"<pre><code>int_fxn(t, y, args)\n</code></pre> <p>Integrates a function.</p> <p>Args:     t (jnp.array): Time.     y (jnp.array): Input vector.     args (float): Additional argument.</p> <p>Returns:     jnp.array: Result of the integration.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def int_fxn(\n        t: jnp.array,\n        y: jnp.array,\n        args\n) -&gt; jnp.array:\n    \"\"\"\n    Integrates a function.\n\n    Args:\n        t (jnp.array): Time.\n        y (jnp.array): Input vector.\n        args (float): Additional argument.\n\n    Returns:\n        jnp.array: Result of the integration.\n    \"\"\"\n    return jnp.linalg.norm(args*t)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.predict_float","title":"predict_float","text":"<pre><code>predict_float(param, time)\n</code></pre> <p>Predicts floating point values.</p> <p>Args:     param (jnp.array): Model parameters.     time (jnp.array): Time.</p> <p>Returns:     jnp.array: Predicted values.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def predict_float(\n        param: jnp.array,\n        time: jnp.array\n) -&gt; jnp.array:\n    \"\"\"\n    Predicts floating point values.\n\n    Args:\n        param (jnp.array): Model parameters.\n        time (jnp.array): Time.\n\n    Returns:\n        jnp.array: Predicted values.\n    \"\"\"\n    return jnp.linalg.norm(jnp.matmul(jnp.array([time]), param))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.solve","title":"solve","text":"<pre><code>solve(path_)\n</code></pre> <p>Solves the ordinary differential equation.</p> <p>Args:     path_ (Func): Function representing the vector field.</p> <p>Returns:     jnp.array: Solution of the differential equation.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def solve(path_: Func) -&gt; jnp.array:\n    \"\"\"\n    Solves the ordinary differential equation.\n\n    Args:\n        path_ (Func): Function representing the vector field.\n\n    Returns:\n        jnp.array: Solution of the differential equation.\n    \"\"\"\n    t0 = 0\n    t1 = 1.\n    #ts = jnp.linspace(t0, t1, len(points))\n    #coeffs = backward_hermite_coefficients(ts, points)\n    #interp = CubicInterpolation(ts, coeffs)\n    #term = ODETerm(vector_field2)\n    #test_fxn = lambda t, y, args : jnp.linalg.norm(args)\n    #term = ODETerm(test_fxn)\n    term = ODETerm(path_)\n    solver = Tsit5()\n    dt0 = None,\n    y0 = 0.\n    #sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=interp)\n    sol = diffeqsolve(term, solver, t0, t1, dt0, y0)\n    (y1,) = sol.ys\n    return y1\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/example.html#chemistry_MEP_TS_optimization.playground.example.vector_field2","title":"vector_field2","text":"<pre><code>vector_field2(t, y, interp)\n</code></pre> <p>Defines the vector field for the ordinary differential equation.</p> <p>Args:     t (jnp.array): Time.     y (jnp.array): Input vector.     interp (CubicInterpolation): Interpolation object.</p> <p>Returns:     jnp.array: Result of the vector field computation.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/example.py</code> <pre><code>def vector_field2(\n        t: jnp.array,\n        y: jnp.array,\n        interp: CubicInterpolation\n) -&gt; jnp.array:\n    \"\"\"\n    Defines the vector field for the ordinary differential equation.\n\n    Args:\n        t (jnp.array): Time.\n        y (jnp.array): Input vector.\n        interp (CubicInterpolation): Interpolation object.\n\n    Returns:\n        jnp.array: Result of the vector field computation.\n    \"\"\"\n    return -y + interp.evaluate(t)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/split_diff_static.html","title":"split_diff_static","text":""},{"location":"reference/chemistry_MEP_TS_optimization/playground/split_diff_static.html#chemistry_MEP_TS_optimization.playground.split_diff_static.dataloader","title":"dataloader","text":"<pre><code>dataloader(arrays, batch_size, *, key)\n</code></pre> <p>Generate batches of data.</p> <p>Args:     arrays (tuple): Tuple of input and output data arrays.     batch_size (int): Size of each batch.     key (jnp.ndarray): Random key for shuffling.</p> <p>Yields:     tuple: Batch of input and output data arrays.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/split_diff_static.py</code> <pre><code>def dataloader(\n        arrays: tuple,\n        batch_size: int,\n        *,\n        key: jnp.ndarray\n) -&gt; tuple:\n    \"\"\"\n    Generate batches of data.\n\n    Args:\n        arrays (tuple): Tuple of input and output data arrays.\n        batch_size (int): Size of each batch.\n        key (jnp.ndarray): Random key for shuffling.\n\n    Yields:\n        tuple: Batch of input and output data arrays.\n    \"\"\"\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jrandom.permutation(key, indices)\n        (key,) = jrandom.split(key, 1)\n        start = 0\n        end = batch_size\n        while end &lt; dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/split_diff_static.html#chemistry_MEP_TS_optimization.playground.split_diff_static.get_data","title":"get_data","text":"<pre><code>get_data(dataset_size, *, key)\n</code></pre> <p>Generate toy dataset.</p> <p>Args:     dataset_size (int): Size of the dataset.     key (jnp.ndarray): Random key for data generation.</p> <p>Returns:     tuple: Tuple of input and output data arrays.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/split_diff_static.py</code> <pre><code>def get_data(\n        dataset_size: int,\n        *,\n        key: jnp.ndarray\n) -&gt; tuple:\n    \"\"\"\n    Generate toy dataset.\n\n    Args:\n        dataset_size (int): Size of the dataset.\n        key (jnp.ndarray): Random key for data generation.\n\n    Returns:\n        tuple: Tuple of input and output data arrays.\n    \"\"\"\n    x = jrandom.normal(key, (dataset_size, 1))\n    y = 5 * x - 2\n    return x, y\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/split_diff_static.html#chemistry_MEP_TS_optimization.playground.split_diff_static.main","title":"main","text":"<pre><code>main(dataset_size=10000, batch_size=256, learning_rate=0.003, steps=1000, width_size=8, depth=1, seed=5678)\n</code></pre> <p>Main function for training a neural network model.</p> <p>Args:     dataset_size (int, optional): Size of the dataset. Defaults to 10000.     batch_size (int, optional): Size of each batch. Defaults to 256.     learning_rate (float, optional): Learning rate. Defaults to 3e-3.     steps (int, optional): Number of training steps. Defaults to 1000.     width_size (int, optional): Width of the MLP layers. Defaults to 8.     depth (int, optional): Depth of the MLP. Defaults to 1.     seed (int, optional): Random seed. Defaults to 5678.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/split_diff_static.py</code> <pre><code>def main(\n    dataset_size: int = 10000,\n    batch_size: int = 256,\n    learning_rate: float = 3e-3,\n    steps: int = 1000,\n    width_size: int = 8,\n    depth: int = 1,\n    seed: int = 5678,\n) -&gt; None:\n    \"\"\"\n    Main function for training a neural network model.\n\n    Args:\n        dataset_size (int, optional): Size of the dataset. Defaults to 10000.\n        batch_size (int, optional): Size of each batch. Defaults to 256.\n        learning_rate (float, optional): Learning rate. Defaults to 3e-3.\n        steps (int, optional): Number of training steps. Defaults to 1000.\n        width_size (int, optional): Width of the MLP layers. Defaults to 8.\n        depth (int, optional): Depth of the MLP. Defaults to 1.\n        seed (int, optional): Random seed. Defaults to 5678.\n    \"\"\"\n    data_key, loader_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 3)\n    data = get_data(dataset_size, key=data_key)\n    data_iter = dataloader(data, batch_size, key=loader_key)\n\n    # Step 1\n    model = eqx.nn.MLP(\n        in_size=1, out_size=1, width_size=width_size, depth=depth, key=model_key\n    )\n\n    # Step 2\n    filter_spec = jtu.tree_map(lambda _: False, model)\n    print(\"initial filter_spec\\n\", filter_spec)\n    filter_spec = eqx.tree_at(\n        lambda tree: (tree.layers[-1].weight, tree.layers[-1].bias),\n        filter_spec,\n        replace=(True, True),\n    )\n    print(\"final filter_spec\\n\", filter_spec)\n\n    diff_model, static_model = eqx.partition(model, filter_spec)\n    print(\"diff model\\n\", diff_model)\n    print(\"static model\\n\", static_model)\n\n    # Step 3\n    @eqx.filter_jit\n    def make_step(model, x, y, opt_state):\n        @eqx.filter_grad\n        def loss(diff_model, static_model, x, y):\n            model = eqx.combine(diff_model, static_model)\n            pred_y = jax.vmap(model)(x)\n            return jnp.mean((y - pred_y) ** 2)\n\n        diff_model, static_model = eqx.partition(model, filter_spec)\n        grads = loss(diff_model, static_model, x, y)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return model, opt_state\n\n    # And now let's train for a short while -- in exactly the usual way -- and see what\n    # happens. We keep the original model around to compare to later.\n    original_model = model\n    optim = optax.sgd(learning_rate)\n    opt_state = optim.init(model)\n    for step, (x, y) in zip(range(steps), data_iter):\n        model, opt_state = make_step(model, x, y, opt_state)\n    print(\n        f\"Parameters of first layer at initialisation:\\n\"\n        f\"{jtu.tree_leaves(original_model.layers[0])}\\n\"\n    )\n    print(\n        f\"Parameters of first layer at end of training:\\n\"\n        f\"{jtu.tree_leaves(model.layers[0])}\\n\"\n    )\n    print(\n        f\"Parameters of last layer at initialisation:\\n\"\n        f\"{jtu.tree_leaves(original_model.layers[-1])}\\n\"\n    )\n    print(\n        f\"Parameters of last layer at end of training:\\n\"\n        f\"{jtu.tree_leaves(model.layers[-1])}\\n\"\n    )\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/test_integrator.html","title":"test_integrator","text":""},{"location":"reference/chemistry_MEP_TS_optimization/playground/test_integrator.html#chemistry_MEP_TS_optimization.playground.test_integrator.integrate","title":"integrate","text":"<pre><code>integrate(params)\n</code></pre> <p>Integrate the given function.</p> <p>Args:     params (jnp.ndarray): Parameters.</p> <p>Returns:     float: Result of integration.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/test_integrator.py</code> <pre><code>def integrate(params: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Integrate the given function.\n\n    Args:\n        params (jnp.ndarray): Parameters.\n\n    Returns:\n        float: Result of integration.\n    \"\"\"\n    t0 = 0.\n    t1 = 1.\n    solver = Tsit5()\n    test_fxn = lambda t, y, *args : jnp\n    term = ODETerm(int_fxn)\n    solution = diffeqsolve(\n        term, solver, t0, t1, dt0=0.1, y0=0, args=params\n    )\n    (y1,) = solution.ys\n    return y1\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/test_integrator.html#chemistry_MEP_TS_optimization.playground.test_integrator.potential","title":"potential","text":"<pre><code>potential(point)\n</code></pre> <p>Calculate the potential energy of the system at a point.</p> <p>Args:     point (jnp.ndarray): Input point.</p> <p>Returns:     float: Potential energy.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/test_integrator.py</code> <pre><code>def potential(point: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the potential energy of the system at a point.\n\n    Args:\n        point (jnp.ndarray): Input point.\n\n    Returns:\n        float: Potential energy.\n    \"\"\"\n    return jnp.sum(point**2)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/test_integrator.html#chemistry_MEP_TS_optimization.playground.test_integrator.predict","title":"predict","text":"<pre><code>predict(param, time)\n</code></pre> <p>Predict the position of a point at a given time.</p> <p>Args:     param (jnp.ndarray): Parameters.     time (float): Time.</p> <p>Returns:     jnp.ndarray: Predicted position.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/test_integrator.py</code> <pre><code>def predict(param: jnp.ndarray, time: float) -&gt; jnp.ndarray:\n    \"\"\"\n    Predict the position of a point at a given time.\n\n    Args:\n        param (jnp.ndarray): Parameters.\n        time (float): Time.\n\n    Returns:\n        jnp.ndarray: Predicted position.\n    \"\"\"\n    return jnp.matmul(jnp.array([time]), param)\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/playground/test_integrator.html#chemistry_MEP_TS_optimization.playground.test_integrator.predict_float","title":"predict_float","text":"<pre><code>predict_float(param, time)\n</code></pre> <p>Predict the position of a point at a given time and return its norm.</p> <p>Args:     param (jnp.ndarray): Parameters.     time (float): Time.</p> <p>Returns:     float: Norm of the predicted position.</p> Source code in <code>chemistry_MEP_TS_optimization/playground/test_integrator.py</code> <pre><code>def predict_float(param: jnp.ndarray, time: float) -&gt; float:\n    \"\"\"\n    Predict the position of a point at a given time and return its norm.\n\n    Args:\n        param (jnp.ndarray): Parameters.\n        time (float): Time.\n\n    Returns:\n        float: Norm of the predicted position.\n    \"\"\"\n    print(\"test\", param, time)\n    return jnp.linalg.norm(jnp.matmul(jnp.array([time]), param))\n</code></pre>"},{"location":"reference/chemistry_MEP_TS_optimization/potentials/base_class.html","title":"base_class","text":""},{"location":"reference/chemistry_MEP_TS_optimization/potentials/constant.html","title":"constant","text":""},{"location":"reference/chemistry_MEP_TS_optimization/potentials/muller_brown.html","title":"muller_brown","text":""},{"location":"reference/chemistry_MEP_TS_optimization/potentials/wolfe_schlegel.html","title":"wolfe_schlegel","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/arg_parser.html","title":"arg_parser","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/configs.html","title":"configs","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/integrator.html","title":"integrator","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/logging.html","title":"logging","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/metrics.html","title":"metrics","text":""},{"location":"reference/chemistry_MEP_TS_optimization/tools/visualize.html","title":"visualize","text":""}]}